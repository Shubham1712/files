
Links:-

**AWS SDK for Java - https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/welcome.html

https://docs.aws.amazon.com/lambda/latest/dg/with-sqs-create-package.html#with-sqs-example-deployment-pkg-java
http://codeomitted.com/upload-file-to-s3-with-spring/
https://www.baeldung.com/spring-cloud-aws-messaging
Creating Clients (DynamoDB)- https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/creating-clients.html

https://medium.com/financial-engines-techblog/aws-sqs-events-on-aws-lambda-61ecb49afec1


https://d1.awsstatic.com/whitepapers/microservices-on-aws.pdf
https://d0.awsstatic.com/whitepapers/DevOps/practicing-continuous-integration-continuous-delivery-on-AWS.pdf

AWS issues - https://github.com/aws/aws-sdk-java/issues/1324


AWS to spring connectivity.. DynamoDB, SQS, SNS, S3 Bucket, cloudformation, lambda ani java 8 streams 
ewdha sagla use kelay 
base is spring boot.. 

1. Elastic Bean Stalk
2. CloudWatch
3. Budget-
	1. Used to monitor budget.


EC2 
- server, elastic compute, instance, bill depends on time you have utilize
- Two storage options : EBS and instance store
	EBS(elastic block store)
	Instance store
	- 
	- Takes memory from host
	- Fast
	- Not reliable
-Preconfigured tablets are available known as Amazon Machine Image.
-By default, when you create EC2 account with Amazon, your account is limited to a max of 20 instances per EC2 region with two default high I/O instances. To 
 extends, has to request to amazon.
 
Types of EC2 instances-
-General purpose: Balanced memory & CPU.
-Compute Optimized: More CPU then RAM
-Memory Optimized: More RAM
-Accelerated computing/GPU: Graphics Optimizes
-Storage optimized: Low latency
-High memory Optimized: High RAM, Nitro System.


1. SQS-
	Amazon SQS is a web service that gives you access to a message queue that can be used to store messages while waiting for a computer to process them.
	1. Used to communicate between microservices.
	2. Default msg retention period is 4 days.
	3. We can change the retention period from 60 secs to 14 days.
	4. Types of Queue - Standard Queue, FIFO Queue
	5. Standard Queue - Default queue type, support unlimited no. of transactions per sec (TPS).
	6. FIFO Queue have all the capabilities of standard queue.
	7. FIFO Queue is used when the ordering of operations and events is critical. EX. Display product prices by sending the price modifications in right order.
	8. FIFO supports limited no. of TPS.
	9. FIFO supports upto 3,000 msgs per sec with batching.
	10. FIFO supports upto 300 msgs per sec action.
	
2. SQS Architecture-
	1. Three main distributed parts-
		- your distributed system ex. microservices.
		- your queue (distributed on Amazon SQS servers)
		- the messages in queue
		
3. SQS key points -
	- To create FIFO Queue, you have to give .fifo extension.
	- You can create Standard Queue in ALL regions.
	- YOu can't change queue type after creating it.
	- Default Visibility Timeout - It is time in which if a message goes to service then for that time period it will be invisible for other services.
	- One of the great thing about SQS is that data is queued in the SQS even if the application server crashes,
	
4. Dead-Letter-Queue-
	-
	
5. DynamoDB -

	Scan Table -
		Data Retrieval -
			-A scan operation reads every item in table or a secondary index.
			-By default scan operation returns all of the data attributes for every item int he table and index.
		Read Consistency-
			-
		ResultSet -
			- 1MB
		Filter Expression-
	
	Query Table -
		Data Retrieval- The query operator finds items based on the primary key value.
		Read Consistency-
			-
		ResultSet -
			- 1MB
		Filter Expression-
		
	
6. Auto Scaling Group-
	
	Commands to be put under User data field in Advanced Details under Configure details of ASG:
	
	#!/bin/bash
	#install httpd (Linus 2 version)
	yum update -y
	yum install -y httpd.*86_64
	systemctl start httpd.service
	systemctl enable httpd.service
	echo "Hello World from $(hostname -f)" > /var/www/html/index.html
	
	
7. AWS Lambda-
	- aws-lambda-java-core -  This library provides the Context object, RequestStreamHandler, and the RequestHandler interfaces. The Context object (AWS Lambda Context Object in Java) provides runtime information about your Lambda function. The predefined interfaces provide one way of defining your Lambda function handler.
	
	- aws-lambda-java-events - This library provides predefined types that you can use when writing Lambda functions to process events published by Amazon S3, Kinesis, Amazon SNS, and Amazon Cognito. These classes help you process the event without having to write your own custom serialization logic.
	
	- Custom Appender for Log4j2.8 – You can use the custom Log4j (see Apache Log4j 2) appender provided by AWS Lambda for logging from your lambda functions. Every call to Log4j methods, such as log.info() or log.error(), will result in a CloudWatch Logs event. The custom appender is called LambdaAppender and must be used in the log4j2.xml file. You must include the aws-lambda-java-log4j2 artifact (artifactId:aws-lambda-java-log4j2) in the deployment package (.jar file).
	
	- LambdaLogger - Lambda provides a logger object that you can retrieve from the context object. LambdaLogger supports multi-line logs.
	If you log a string that includes line breaks with System.out.println, each line break results in a separate entry in CloudWatch Logs. If you use LambdaLogger, you get one entry with multiple lines.
	
	- You have to add the Apache maven-shade-plugin to create the standalone .jar.
	 
	- You can create Role for your Lambda(eg. to access CloudWatch) under Services -> IAM Console -> Role.
	
	- AWS Lambda With Java
		- Dependency - com.amazonaws:aws-lambda-java-core
					 - com.amazonaws:aws-lambda-java-events
					 - org.apache.maven.plugins:maven-shade-plugin
		- You have to implement RequestHandler(input (String/object) , context object) and RequestSreamHandler (parameter are InputStream, 	ObjectStream and Context objects)
		- Add role to Lambda to access aws resources
		- Create jar file and upload to Lambda and in Handler add "package_name.class_name::methodName"
		- You can run it by clicking on Test button.
	
8. aws-java-sdk-bom
	-  There may be situation that aws-java-sdk-ec2 module is pulling in an older version of aws-java-sdk-core. This intermixing of different versions of SDK modules can create unexpected issues. To ensure that Maven pulls in the correct version of the dependencies, import the aws-java-sdk-bom into your dependency management section and specify your project’s dependencies.
	- When you are importing a BOM, you will need to mention the type as pom and the scope as import.
	
9. message GroupId
	- The message group ID is the tag that specifies that a message belongs to a specific message group. Messages that belong to the same message group are always processed one by one, in a strict order relative to the message group.
	- Used in fiff queue
	
10. SNS-
	-After your subscription is created, you must confirm it. Only HTTP/S endpoints, email addresses, and AWS resources in other AWS accounts require confirmation. (Amazon SQS queues and Lambda functions in the same AWS account—as well as mobile endpoints —don't require confirmation.)
	-MessageAttribute
		-Name, type, and value must not be empty or null. In addition, the message body should not be empty or null. All parts of the message attribute, including name, type, and value, are included in the message size restriction, which is 256 KB.
		
	- To Configure Dead letter Queue to SNS		
		- Currently, you can't use an Amazon SQS FIFO queue as a dead-letter queue for an Amazon SNS subscription.
		- https://docs.aws.amazon.com/sns/latest/dg/sns-configure-dead-letter-queue.html  (java code)
		
	- SNS vs SQS
		- Amazon SNS allows applications to send time-critical messages to multiple subscribers through a “push” mechanism, eliminating the need to periodically check or “poll” for updates like SQS.
		- A common pattern is to use SNS to publish messages to Amazon SQS queues to reliably send messages to one or many system components asynchronously.

	- Can I get a history of SNS API calls made on my account for security analysis
		- Yes. SNS supports AWS CloudTrail, a web service that records AWS API calls for your account and delivers log files to you. 
		
	- SNS ARN 
		- arn:aws:sns:us-east-1:1234567890123456:mytopic
		
11. AWS DynamoDB 
	- In DynamoDB, tables, items(400KB in size), and attributes are the core components that you work with. A table is a collection of items, and each item is a collection of attributes. 
	- The partition key of an item is also known as its hash attribute. The term hash attribute derives from the use of an internal hash function in DynamoDB that evenly distributes data items across partitions, based on their partition key values
	- The sort key of an item is also known as its range attribute. The term range attribute derives from the way DynamoDB stores items with the same partition key physically close together, in sorted order by the sort key value.
	
	- Secondary Indexes
		- You can create one or more secondary indexes on a table. A secondary index lets you query the data in the table using an alternate key, in addition to queries against the primary key
		- Global secondary index (GSI)
			– An index with a partition key and sort key that can be different from those on the table.
		- Local secondary index (LSI)
			– An index that has the same partition key as the table, but a different sort key.
			
	- Each table in DynamoDB has a limit of 20 global secondary indexes (default limit) and 5 local secondary indexes per table.
	
	- DynamoDB Streams
		- If you enable DynamoDB Streams on a table, you can associate the stream Amazon Resource Name (ARN) with an AWS Lambda function that you write. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes your Lambda function synchronously when it detects new stream records.
		- DynamoDB Streams is an optional feature that captures data modification events in DynamoDB tables. The data about these events appear in the stream in near-real time, and in the order that the events occurred.
		- Each event is represented by a stream record. If you enable a stream on a table, DynamoDB Streams writes a stream record whenever one of the following events occurs:
			- A new item is added to the table:- The stream captures an image of the entire item, including all of its attributes.
			- An item is updated:- The stream captures the "before" and "after" image of any attributes that were modified in the item.
			- An item is deleted from the table:- The stream captures an image of the entire item before it was deleted.
			
		- DynamoDB API
			- Control Plane (createTable, listTable, updateTable etc.)
			- Data Plane (CRUD opaerations - getItem, puItem, updateItem, BatchWriteItem, BatchGetItem, Query, Scan)
			- DynamoDB Streams (listStream, describeStream, getRecords)
			- Transactions (TransactionWriteItems, TransactionGetItems)
			
		- Data Types
			- Scalar types (String limit 400KB, Binary - compressed/encrypted data (400KB)), Document types, Set types.
		
		- Read/Write Capacity Mode
			- on-Demand (read/write per request. Pay as per the requests)
			- on-Provisioned (read/write per second)
			
		- Each stream record also contains the name of the table, the event timestamp, and other metadata. Stream records have a lifetime of 24 hours; after that, they are automatically removed from the stream.
		
		- When item is inserted/update/deleted stream record gets created and passed to DynamoDb streams then it triggers AWS Lambda which contains logic to send mail to new Customers.
		
		-The downloadable version of DynamoDB is intended for development and testing purposes only. By comparison, the DynamoDB web service is a managed service with scalability, availability, and durability features that make it ideal for production use.
		
	- How to start sdk implementation
		- Download DynamoDB Local version
		- path - C:\_Copied from P drive\Shubham\AWS\dynamodb_local_latest
		- java -Djava.library.path=./DynamoDBLocal_lib -jar DynamoDBLocal.jar -sharedDb
		- aws dynamodb list-tables  --endpoint-url http://localhost:8000							(to list tables)
		- aws dynamodb scan --table-name  your_table_name  --endpoint-url http://localhost:8000		 (to retrieve value from table)
		
	- Low-Level Interfaces
		-Every language-specific AWS SDK provides a low-level interface for Amazon DynamoDB, with methods that closely resemble low-level DynamoDB API requests.
		- such as S for string or N for number.
	
	- AmazonServiceException
		—Thrown if the client request was correctly transmitted to DynamoDB, but DynamoDB could not process the request and returned an error response instead.

	- AmazonClientException
		—Thrown if the client could not get a response from a service, or if the client could not parse the response from a service.
		
	- DynamoDBMapper
		- The AWS SDK for Java provides a DynamoDBMapper class, allowing you to map your client-side classes to Amazon DynamoDB tables. To use DynamoDBMapper, you define the relationship between items in a DynamoDB table and their corresponding object instances in your code. The DynamoDBMapper class enables you to access your tables; perform various create, read, update, and delete (CRUD) operations; and execute queries.
		
	- DynamoDBDocument
		- For example, suppose that you wanted to map a JSON document to a DynamoDB attribute of type Map (M). The following code example defines an item containing a nested attribute (Pictures) of type Map.
		
	- DynamoDBIndexHashKey
		- Maps a class property to the partition key of a global secondary index. The property must be one of the scalar string, number, or binary types. The property can't be a collection type.
		
	- Projection Expressions
		- To read data from a table, you use operations such as GetItem, Query, or Scan. Amazon DynamoDB returns all the item attributes by default. To get only some, rather than all of the attributes, use a projection expression.
		
	- Expression Attribute Names
		- An expression attribute name is a placeholder that you use in an Amazon DynamoDB expression as an alternative to an actual attribute name. An expression attribute name must begin with a pound sign (#), and be followed by one or more alphanumeric characters.
		
	- Expression Attribute Values
		- If you need to compare an attribute with a value, define an expression attribute value as a placeholder. Expression attribute values in Amazon DynamoDB are substitutes for the actual values that you want to compare—values that you might not know until runtime. An expression attribute value must begin with a colon (:) and be followed by one or more alphanumeric characters.
	
	SQL vs NoSQL-

	- Advantages of NoSQL
		- Inertions & retrivels
		- Schema is easily changable
		- Built for Scale
		- Built for aggreagations
		
	- Disadvantages of NoSQL
		- Not built for updates (consistency (ACID))
		- Readtime are slower (eg. retrieving particular attribute value eg. age)
		- Relations are not implicit
		- Joins are bad.
	
Cloud Formation-
	- Save the template locally or in an S3 bucket. If you created a template, save it with any file extension like .json, .yaml, .template or .txt.
	- Architecture
		- AWSTemplateFormatVersion
		- Description  (1024 bytes)
		- Metadata
		- Parameters ( 60 parameters, Parameter name is 255 characters)
		- Mappings  (max 100 mappings, Mapping Attribute 64 attributes, Attribute size is 255 characters)
		- Conditions 
		- Transforms
		- Resources (Required, 200 resources, Resource name 255 characters)
		- Outputs
	- Stacks (200 stacks)
	- Stack Set (100 stack sets)
	- Stack Instances ( 2,000 stack instances per stack set)
	- After all the resources have been created, AWS CloudFormation reports that your stack has been created. 
	- If stack creation fails, AWS CloudFormation rolls back your changes by deleting the resources that it created.
	- Execute the change set (update/ modify template) that you want to apply to your stack. AWS CloudFormation updates your stack by updating only the resources that you modified and signals that your stack has been successfully updated.
	- You can delete the stack manually using aws console.
	- If you want to delete a stack but want to retain some resources in that stack, you can use a deletion policy to retain those resources.
	
	-Service Role
		- A service role is an AWS Identity and Access Management (IAM) role that allows AWS CloudFormation to make calls to resources in a stack on your behalf.
		- You create the service role and its permission policy with the IAM service. 
		- To associate a service role with a stack, specify the role when you create the stack. 
		
	- Logging AWS CloudFormation API Calls with AWS CloudTrail
		- AWS CloudFormation is integrated with AWS CloudTrail, a service that provides a record of actions taken by a user, role, or an AWS service in AWS CloudFormation. 
		
		Note - To execute stack we need to create role and assigned it to CF (which includes the permission to access the resoucres mentioned in the stack and also IAM role policy(inline policy) which can create roles given in stacks)
		
Elastic BeanStalk-
	- 
	
Amazon EC2-
	- Using Amazon EC2 eliminates your need to invest in hardware up front, so you can develop and deploy applications faster.
	- Virtual computing environments, known as instances
	- Preconfigured templates for your instances, known as Amazon Machine Images (AMIs), 
	- Various configurations of CPU, memory, storage, and networking capacity for your instances, known as instance types
	- 
	
	-Security Groups
		- You can use multiple security group for instance, But it is not recommended.

	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	